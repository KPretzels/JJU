{"cells":[{"cell_type":"markdown","id":"e00f269d","metadata":{"id":"e00f269d"},"source":["# SemanticChunker\n","\n","텍스트를 의미론적 유사성에 기반하여 분할합니다.\n","\n","**Reference**\n","\n","- [Greg Kamradt의 노트북](https://github.com/FullStackRetrieval-com/RetrievalTutorials/blob/main/tutorials/LevelsOfTextSplitting/5_Levels_Of_Text_Splitting.ipynb)\n","\n","이 방법은 텍스트를 문장 단위로 분할한 후, 3개의 문장씩 그룹화하고, 임베딩 공간에서 유사한 문장들을 병합하는 과정을 거칩니다.\n"]},{"cell_type":"markdown","id":"90b48cce","metadata":{"id":"90b48cce"},"source":["샘플 텍스트를 로드하고 내용을 출력합니다.\n"]},{"cell_type":"code","execution_count":19,"id":"c170dd43","metadata":{"id":"c170dd43","executionInfo":{"status":"ok","timestamp":1735346999857,"user_tz":-540,"elapsed":407,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}}},"outputs":[],"source":["OPENAI_API_KEY = \"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\""]},{"cell_type":"markdown","id":"e1817a14","metadata":{"id":"e1817a14"},"source":["## SemanticChunker 생성\n","\n","`SemanticChunker`는 LangChain의 실험적 기능 중 하나로, 텍스트를 의미론적으로 유사한 청크로 분할하는 역할을 합니다.\n","\n","이를 통해 텍스트 데이터를 보다 효과적으로 처리하고 분석할 수 있습니다.\n"]},{"cell_type":"code","execution_count":null,"id":"b06b68b4","metadata":{"id":"b06b68b4"},"outputs":[],"source":["# # API 키를 환경변수로 관리하기 위한 설정 파일\n","# from dotenv import load_dotenv\n","\n","# # API 키 정보 로드\n","# load_dotenv()"]},{"cell_type":"markdown","id":"ab33ae70","metadata":{"id":"ab33ae70"},"source":["`SemanticChunker`를 사용하여 텍스트를 의미적으로 관련된 청크로 분할합니다.\n"]},{"cell_type":"code","source":["pip install -q langchain_experimental"],"metadata":{"id":"YelmCKDnrNFK","executionInfo":{"status":"ok","timestamp":1735345918657,"user_tz":-540,"elapsed":3397,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}}},"id":"YelmCKDnrNFK","execution_count":3,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oIrk_bxErwGo","executionInfo":{"status":"ok","timestamp":1735346054521,"user_tz":-540,"elapsed":20810,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}},"outputId":"125aec96-e22d-4699-b3f3-12729f0f37ee"},"id":"oIrk_bxErwGo","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["pip install -q langchain_openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KENyN0Zns7NV","executionInfo":{"status":"ok","timestamp":1735346351526,"user_tz":-540,"elapsed":5529,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}},"outputId":"191acce5-5854-48df-ae79-5d29318a445d"},"id":"KENyN0Zns7NV","execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting langchain_openai\n","  Downloading langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.27 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.3.28)\n","Collecting openai<2.0.0,>=1.58.1 (from langchain_openai)\n","  Downloading openai-1.58.1-py3-none-any.whl.metadata (27 kB)\n","Collecting tiktoken<1,>=0.7 (from langchain_openai)\n","  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (6.0.2)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (1.33)\n","Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (0.2.3)\n","Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (24.2)\n","Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (2.10.3)\n","Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (9.0.0)\n","Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.27->langchain_openai) (4.12.2)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (3.7.1)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.9.0)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.28.1)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (0.8.2)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (1.3.1)\n","Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain_openai) (4.67.1)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n","Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (3.10)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.2.2)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (2024.12.14)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (1.0.7)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.58.1->langchain_openai) (0.14.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.27->langchain_openai) (3.0.0)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_openai) (3.10.12)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4.0,>=0.3.27->langchain_openai) (1.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain_openai) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.27->langchain_openai) (2.27.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.0)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.2.3)\n","Downloading langchain_openai-0.2.14-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openai-1.58.1-py3-none-any.whl (454 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.3/454.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: tiktoken, openai, langchain_openai\n","  Attempting uninstall: openai\n","    Found existing installation: openai 1.57.4\n","    Uninstalling openai-1.57.4:\n","      Successfully uninstalled openai-1.57.4\n","Successfully installed langchain_openai-0.2.14 openai-1.58.1 tiktoken-0.8.0\n"]}]},{"cell_type":"code","execution_count":14,"id":"312e3aae","metadata":{"id":"312e3aae","executionInfo":{"status":"ok","timestamp":1735346526652,"user_tz":-540,"elapsed":507,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}}},"outputs":[],"source":["from langchain_experimental.text_splitter import SemanticChunker\n","from langchain_openai.embeddings import OpenAIEmbeddings\n","\n","# OpenAI 임베딩을 사용하여 의미론적 청크 분할기를 초기화합니다.\n","text_splitter = SemanticChunker(OpenAIEmbeddings(api_key='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'))"]},{"cell_type":"markdown","id":"dab515b0","metadata":{"id":"dab515b0"},"source":["## 텍스트 분할\n"]},{"cell_type":"markdown","id":"b0c9b20b","metadata":{"id":"b0c9b20b"},"source":["- `text_splitter`를 사용하여 `file` 텍스트를 문서 단위로 분할합니다.\n"]},{"cell_type":"code","execution_count":15,"id":"dfb5870d","metadata":{"id":"dfb5870d","executionInfo":{"status":"ok","timestamp":1735346536787,"user_tz":-540,"elapsed":3114,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}}},"outputs":[],"source":["chunks = text_splitter.split_text(file)"]},{"cell_type":"markdown","id":"14a777bc","metadata":{"id":"14a777bc"},"source":["분할된 청크를 확인합니다.\n"]},{"cell_type":"code","execution_count":16,"id":"eec69bff","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eec69bff","executionInfo":{"status":"ok","timestamp":1735346543547,"user_tz":-540,"elapsed":401,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}},"outputId":"107cc9c9-34d2-4dc1-e337-e966aa90e930"},"outputs":[{"output_type":"stream","name":"stdout","text":["Semantic Search\n","\n","정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n","\n","Embedding\n","\n","정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n","\n","Token\n","\n","정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n","\n","Tokenizer\n","\n","정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다. 예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n","\n","VectorStore\n","\n","정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다. 예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n","\n","SQL\n","\n","정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다. 예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리\n","\n","CSV\n","\n","정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다. 예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다. 연관키워드: 데이터 형식, 파일 처리, 데이터 교환\n","\n","JSON\n","\n","정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다. 예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다. 연관키워드: 데이터 교환, 웹 개발, API\n","\n","Transformer\n","\n","정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다. 예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다. 연관키워드: 딥러닝, 자연어 처리, Attention\n","\n","HuggingFace\n","\n","정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다. 예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다. 연관키워드: 자연어 처리, 딥러닝, 라이브러리\n","\n","Digital Transformation\n","\n","정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다. 예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다. 연관키워드: 혁신, 기술, 비즈니스 모델\n","\n","Crawling\n","\n","정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다. 예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다. 연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n","\n","Word2Vec\n","\n","정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n"]}],"source":["# 분할된 청크 중 첫 번째 청크를 출력합니다.\n","print(chunks[0])"]},{"cell_type":"markdown","id":"8f03b26b","metadata":{"id":"8f03b26b"},"source":["`create_documents()` 함수를 사용하여 청크를 문서로 변환할 수 있습니다.\n"]},{"cell_type":"code","execution_count":17,"id":"fadaf823","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fadaf823","executionInfo":{"status":"ok","timestamp":1735346580315,"user_tz":-540,"elapsed":1738,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}},"outputId":"205a385f-4e86-4e44-911d-7291034c5654"},"outputs":[{"output_type":"stream","name":"stdout","text":["Semantic Search\n","\n","정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n","\n","Embedding\n","\n","정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n","\n","Token\n","\n","정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n","\n","Tokenizer\n","\n","정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다. 예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n","\n","VectorStore\n","\n","정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다. 예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n","\n","SQL\n","\n","정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다. 예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리\n","\n","CSV\n","\n","정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다. 예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다. 연관키워드: 데이터 형식, 파일 처리, 데이터 교환\n","\n","JSON\n","\n","정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다. 예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다. 연관키워드: 데이터 교환, 웹 개발, API\n","\n","Transformer\n","\n","정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다. 예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다. 연관키워드: 딥러닝, 자연어 처리, Attention\n","\n","HuggingFace\n","\n","정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다. 예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다. 연관키워드: 자연어 처리, 딥러닝, 라이브러리\n","\n","Digital Transformation\n","\n","정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다. 예시: 기업이 클라우드 컴퓨팅을 도입하여 데이터 저장과 처리를 혁신하는 것은 디지털 변환의 예입니다. 연관키워드: 혁신, 기술, 비즈니스 모델\n","\n","Crawling\n","\n","정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다. 예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다. 연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n","\n","Word2Vec\n","\n","정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n"]}],"source":["# text_splitter를 사용하여 분할합니다.\n","docs = text_splitter.create_documents([file])\n","print(docs[0].page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다."]},{"cell_type":"markdown","id":"1633cf8e","metadata":{"id":"1633cf8e"},"source":["## Breakpoints\n","\n","이 chunker는 문장을 \"분리\"할 시점을 결정하여 작동합니다. 이는 두 문장 간의 임베딩 차이를 살펴봄으로써 이루어집니다.\n","\n","그 차이가 특정 임계값을 넘으면 문장이 분리됩니다.\n","\n","- 참고 영상: https://youtu.be/8OJC21T2SL4?si=PzUtNGYJ_KULq3-w&t=2580\n","\n","### Percentile\n","\n","기본적인 분리 방식은 백분위수(`Percentile`) 를 기반으로 합니다.\n","\n","이 방법에서는 문장 간의 모든 차이를 계산한 다음, 지정한 백분위수를 기준으로 분리합니다.\n"]},{"cell_type":"code","execution_count":20,"id":"744bbd95","metadata":{"id":"744bbd95","executionInfo":{"status":"ok","timestamp":1735347032151,"user_tz":-540,"elapsed":374,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}}},"outputs":[],"source":["text_splitter = SemanticChunker(\n","    # OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화합니다.\n","    OpenAIEmbeddings(api_key=OPENAI_API_KEY),\n","    # 분할 기준점 유형을 백분위수로 설정합니다.\n","    breakpoint_threshold_type=\"percentile\",\n","    breakpoint_threshold_amount=70,\n",")"]},{"cell_type":"markdown","id":"59aa8318","metadata":{"id":"59aa8318"},"source":["분할된 결과를 확인합니다.\n"]},{"cell_type":"code","execution_count":21,"id":"6c7b3262","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6c7b3262","executionInfo":{"status":"ok","timestamp":1735347038642,"user_tz":-540,"elapsed":1297,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}},"outputId":"aac9880d-a9a5-4abb-ba74-ac988b607904"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Chunk 0]\n","\n","Semantic Search\n","\n","정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n","\n","Embedding\n","\n","정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n","============================================================\n","[Chunk 1]\n","\n","예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n","\n","Token\n","\n","정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다.\n","============================================================\n","[Chunk 2]\n","\n","예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n","\n","Tokenizer\n","\n","정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n","============================================================\n","[Chunk 3]\n","\n","예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n","\n","VectorStore\n","\n","정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n","============================================================\n","[Chunk 4]\n","\n","예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n","\n","SQL\n","\n","정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n","============================================================\n"]}],"source":["docs = text_splitter.create_documents([file])\n","for i, doc in enumerate(docs[:5]):\n","    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n","    print(doc.page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.\n","    print(\"===\" * 20)"]},{"cell_type":"markdown","id":"07e83f74","metadata":{"id":"07e83f74"},"source":["`docs`의 길이를 출력합니다.\n"]},{"cell_type":"code","execution_count":22,"id":"20c0cbd0","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"20c0cbd0","executionInfo":{"status":"ok","timestamp":1735347733088,"user_tz":-540,"elapsed":407,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}},"outputId":"6de38aa5-3bd8-4e5a-f914-fe1eed5c5f0e"},"outputs":[{"output_type":"stream","name":"stdout","text":["27\n"]}],"source":["print(len(docs))  # docs의 길이를 출력합니다."]},{"cell_type":"markdown","id":"21c1c9e8","metadata":{"id":"21c1c9e8"},"source":["### Standard Deviation\n","\n","이 방법에서는 지정한 `breakpoint_threshold_amount` 표준편차보다 큰 차이가 있는 경우 분할됩니다.\n","\n","- `breakpoint_threshold_type` 매개변수를 \"standard_deviation\"으로 설정하여 청크 분할 기준을 표준편차 기반으로 지정합니다.\n"]},{"cell_type":"code","execution_count":24,"id":"16a8d823","metadata":{"id":"16a8d823","executionInfo":{"status":"ok","timestamp":1735347781622,"user_tz":-540,"elapsed":381,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}}},"outputs":[],"source":["text_splitter = SemanticChunker(\n","    # OpenAI의 임베딩 모델을 사용하여 시맨틱 청커를 초기화합니다.\n","    OpenAIEmbeddings(api_key=OPENAI_API_KEY),\n","    # 분할 기준으로 표준 편차를 사용합니다.\n","    breakpoint_threshold_type=\"standard_deviation\",\n","    breakpoint_threshold_amount=1.25,\n",")"]},{"cell_type":"markdown","id":"690db96c","metadata":{"id":"690db96c"},"source":["분할된 결과를 확인합니다.\n"]},{"cell_type":"code","execution_count":25,"id":"1764de39","metadata":{"id":"1764de39","executionInfo":{"status":"ok","timestamp":1735347807606,"user_tz":-540,"elapsed":2005,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}}},"outputs":[],"source":["# text_splitter를 사용하여 분할합니다.\n","docs = text_splitter.create_documents([file])"]},{"cell_type":"code","execution_count":26,"id":"0743d8f6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0743d8f6","executionInfo":{"status":"ok","timestamp":1735347813328,"user_tz":-540,"elapsed":1460,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}},"outputId":"9d590d62-db59-4c01-9c5f-af959eaf68b0"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Chunk 0]\n","\n","Semantic Search\n","\n","정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n","\n","Embedding\n","\n","정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다. 예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n","\n","Token\n","\n","정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n","\n","Tokenizer\n","\n","정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다. 예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n","\n","VectorStore\n","\n","정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n","============================================================\n","[Chunk 1]\n","\n","예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n","\n","SQL\n","\n","정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n","============================================================\n","[Chunk 2]\n","\n","예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리\n","\n","CSV\n","\n","정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다. 예시: 이름, 나이, 직업이라는 헤더를 가진 CSV 파일에는 홍길동, 30, 개발자와 같은 데이터가 포함될 수 있습니다. 연관키워드: 데이터 형식, 파일 처리, 데이터 교환\n","\n","JSON\n","\n","정의: JSON(JavaScript Object Notation)은 경량의 데이터 교환 형식으로, 사람과 기계 모두에게 읽기 쉬운 텍스트를 사용하여 데이터 객체를 표현합니다. 예시: {\"이름\": \"홍길동\", \"나이\": 30, \"직업\": \"개발자\"}는 JSON 형식의 데이터입니다. 연관키워드: 데이터 교환, 웹 개발, API\n","\n","Transformer\n","\n","정의: 트랜스포머는 자연어 처리에서 사용되는 딥러닝 모델의 한 유형으로, 주로 번역, 요약, 텍스트 생성 등에 사용됩니다. 이는 Attention 메커니즘을 기반으로 합니다.\n","============================================================\n","[Chunk 3]\n","\n","예시: 구글 번역기는 트랜스포머 모델을 사용하여 다양한 언어 간의 번역을 수행합니다. 연관키워드: 딥러닝, 자연어 처리, Attention\n","\n","HuggingFace\n","\n","정의: HuggingFace는 자연어 처리를 위한 다양한 사전 훈련된 모델과 도구를 제공하는 라이브러리입니다. 이는 연구자와 개발자들이 쉽게 NLP 작업을 수행할 수 있도록 돕습니다.\n","============================================================\n","[Chunk 4]\n","\n","예시: HuggingFace의 Transformers 라이브러리를 사용하여 감정 분석, 텍스트 생성 등의 작업을 수행할 수 있습니다. 연관키워드: 자연어 처리, 딥러닝, 라이브러리\n","\n","Digital Transformation\n","\n","정의: 디지털 변환은 기술을 활용하여 기업의 서비스, 문화, 운영을 혁신하는 과정입니다. 이는 비즈니스 모델을 개선하고 디지털 기술을 통해 경쟁력을 높이는 데 중점을 둡니다.\n","============================================================\n"]}],"source":["docs = text_splitter.create_documents([file])\n","for i, doc in enumerate(docs[:5]):\n","    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n","    print(doc.page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.\n","    print(\"===\" * 20)"]},{"cell_type":"markdown","id":"095170af","metadata":{"id":"095170af"},"source":["`docs`의 길이를 출력합니다.\n"]},{"cell_type":"code","execution_count":27,"id":"ee9f46ad","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ee9f46ad","executionInfo":{"status":"ok","timestamp":1735347825083,"user_tz":-540,"elapsed":427,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}},"outputId":"6f0f766b-ea39-4c67-ad20-a5337c055163"},"outputs":[{"output_type":"stream","name":"stdout","text":["14\n"]}],"source":["print(len(docs))  # docs의 길이를 출력합니다."]},{"cell_type":"markdown","id":"c5b03d9b","metadata":{"id":"c5b03d9b"},"source":["### Interquartile\n","\n","이 방법에서는 사분위수 범위(interquartile range)를 사용하여 청크를 분할합니다.\n"]},{"cell_type":"markdown","id":"fb408177","metadata":{"id":"fb408177"},"source":["- `breakpoint_threshold_type` 매개변수를 \"interquartile\"로 설정하여 청크 분할 기준을 사분위수 범위로 지정합니다.\n"]},{"cell_type":"code","execution_count":29,"id":"f32f5fe8","metadata":{"id":"f32f5fe8","executionInfo":{"status":"ok","timestamp":1735347854997,"user_tz":-540,"elapsed":436,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}}},"outputs":[],"source":["text_splitter = SemanticChunker(\n","    # OpenAI의 임베딩 모델을 사용하여 의미론적 청크 분할기를 초기화합니다.\n","    OpenAIEmbeddings(api_key=OPENAI_API_KEY),\n","    # 분할 기준점 임계값 유형을 사분위수 범위로 설정합니다.\n","    breakpoint_threshold_type=\"interquartile\",\n","    breakpoint_threshold_amount=0.5,\n",")"]},{"cell_type":"code","execution_count":30,"id":"12e0d2d6","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"12e0d2d6","executionInfo":{"status":"ok","timestamp":1735347860283,"user_tz":-540,"elapsed":1344,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}},"outputId":"802d1761-af03-4f99-8c40-31a71d0dcfef"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Chunk 0]\n","\n","Semantic Search\n","\n","정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다. 예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다. 연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n","\n","Embedding\n","\n","정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n","============================================================\n","[Chunk 1]\n","\n","예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다. 연관키워드: 자연어 처리, 벡터화, 딥러닝\n","\n","Token\n","\n","정의: 토큰은 텍스트를 더 작은 단위로 분할하는 것을 의미합니다. 이는 일반적으로 단어, 문장, 또는 구절일 수 있습니다. 예시: 문장 \"나는 학교에 간다\"를 \"나는\", \"학교에\", \"간다\"로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n","\n","Tokenizer\n","\n","정의: 토크나이저는 텍스트 데이터를 토큰으로 분할하는 도구입니다. 이는 자연어 처리에서 데이터를 전처리하는 데 사용됩니다.\n","============================================================\n","[Chunk 2]\n","\n","예시: \"I love programming.\"이라는 문장을 [\"I\", \"love\", \"programming\", \".\"]으로 분할합니다. 연관키워드: 토큰화, 자연어 처리, 구문 분석\n","\n","VectorStore\n","\n","정의: 벡터스토어는 벡터 형식으로 변환된 데이터를 저장하는 시스템입니다. 이는 검색, 분류 및 기타 데이터 분석 작업에 사용됩니다.\n","============================================================\n","[Chunk 3]\n","\n","예시: 단어 임베딩 벡터들을 데이터베이스에 저장하여 빠르게 접근할 수 있습니다. 연관키워드: 임베딩, 데이터베이스, 벡터화\n","\n","SQL\n","\n","정의: SQL(Structured Query Language)은 데이터베이스에서 데이터를 관리하기 위한 프로그래밍 언어입니다. 데이터 조회, 수정, 삽입, 삭제 등 다양한 작업을 수행할 수 있습니다.\n","============================================================\n","[Chunk 4]\n","\n","예시: SELECT * FROM users WHERE age > 18;은 18세 이상의 사용자 정보를 조회합니다. 연관키워드: 데이터베이스, 쿼리, 데이터 관리\n","\n","CSV\n","\n","정의: CSV(Comma-Separated Values)는 데이터를 저장하는 파일 형식으로, 각 데이터 값은 쉼표로 구분됩니다. 표 형태의 데이터를 간단하게 저장하고 교환할 때 사용됩니다.\n","============================================================\n"]}],"source":["# text_splitter를 사용하여 분할합니다.\n","docs = text_splitter.create_documents([file])\n","\n","# 결과를 출력합니다.\n","for i, doc in enumerate(docs[:5]):\n","    print(f\"[Chunk {i}]\", end=\"\\n\\n\")\n","    print(doc.page_content)  # 분할된 문서 중 첫 번째 문서의 내용을 출력합니다.\n","    print(\"===\" * 20)"]},{"cell_type":"markdown","id":"9d186bb7","metadata":{"id":"9d186bb7"},"source":["`docs`의 길이를 출력합니다.\n"]},{"cell_type":"code","execution_count":31,"id":"3c693c11","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3c693c11","executionInfo":{"status":"ok","timestamp":1735347875252,"user_tz":-540,"elapsed":362,"user":{"displayName":"Chang Jun Lee","userId":"07449846774346066151"}},"outputId":"89243199-8499-4bf9-b2c2-838aca441efc"},"outputs":[{"output_type":"stream","name":"stdout","text":["23\n"]}],"source":["print(len(docs))  # docs의 길이를 출력합니다."]},{"cell_type":"code","execution_count":null,"id":"9dec0348","metadata":{"id":"9dec0348"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"py-test","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}