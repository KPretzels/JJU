{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b294b3be",
   "metadata": {
    "id": "b294b3be"
   },
   "source": [
    "# 벡터스토어 기반 검색기(VectorStore-backed Retriever)\n",
    "\n",
    "**VectorStore 지원 검색기** 는 vector store를 사용하여 문서를 검색하는 retriever입니다.\n",
    "\n",
    "Vector store에 구현된 **유사도 검색(similarity search)** 이나 **MMR** 과 같은 검색 메서드를 사용하여 vector store 내의 텍스트를 쿼리합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc586a1",
   "metadata": {
    "id": "1dc586a1"
   },
   "source": [
    "아래의 코드를 실행하여 VectorStore 를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "137bd65d",
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1737384423904,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "137bd65d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"10-01\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3rF4kAvDKmJO",
   "metadata": {
    "executionInfo": {
     "elapsed": 7102,
     "status": "ok",
     "timestamp": 1737384433632,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "3rF4kAvDKmJO"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "yjSclFvsKuJi",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4672,
     "status": "ok",
     "timestamp": 1737384207744,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "yjSclFvsKuJi",
    "outputId": "1c727a6f-424b-411e-cffa-30b857c27db9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.9/411.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lTFMY3QRK9UB",
   "metadata": {
    "executionInfo": {
     "elapsed": 3740,
     "status": "ok",
     "timestamp": 1737384265688,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "lTFMY3QRK9UB"
   },
   "outputs": [],
   "source": [
    "!pip install -qU langchain_faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "qwwh0QDOLEuS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6309,
     "status": "ok",
     "timestamp": 1737384309830,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "qwwh0QDOLEuS",
    "outputId": "c9fb5244-e437-432b-c421-cfaf4273718a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -qU faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee9b98ed",
   "metadata": {
    "executionInfo": {
     "elapsed": 4148,
     "status": "ok",
     "timestamp": 1737384446982,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "ee9b98ed"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "# TextLoader를 사용하여 파일을 로드합니다.\n",
    "loader = TextLoader(\"/content/data/appendix-keywords.txt\")\n",
    "\n",
    "# 문서를 로드합니다.\n",
    "documents = loader.load()\n",
    "\n",
    "# 문자 기반으로 텍스트를 분할하는 CharacterTextSplitter를 생성합니다. 청크 크기는 300이고 청크 간 중복은 없습니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 로드된 문서를 분할합니다.\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# OpenAI 임베딩을 생성합니다.\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# 분할된 텍스트와 임베딩을 사용하여 FAISS 벡터 데이터베이스를 생성합니다.\n",
    "db = FAISS.from_documents(split_docs, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babc937f",
   "metadata": {
    "id": "babc937f"
   },
   "source": [
    "### VectorStore에서 VectorStoreRetriever 초기화(as_retriever)\n",
    "\n",
    "`as_retriever` 메서드는 VectorStore 객체를 기반으로 VectorStoreRetriever를 초기화하고 반환합니다. 이 메서드를 통해 다양한 검색 옵션을 설정하여 사용자의 요구에 맞는 문서 검색을 수행할 수 있습니다.\n",
    "\n",
    "**매개변수(parameters)**\n",
    "\n",
    "- `**kwargs`: 검색 함수에 전달할 키워드 인자\n",
    "  - `search_type`: 검색 유형 (\"similarity\", \"mmr\", \"similarity_score_threshold\")\n",
    "  - `search_kwargs`: 추가 검색 옵션\n",
    "    - `k`: 반환할 문서 수 (기본값: 4)\n",
    "    - `score_threshold`: similarity_score_threshold 검색의 최소 유사도 임계값\n",
    "    - `fetch_k`: MMR 알고리즘에 전달할 문서 수 (기본값: 20)\n",
    "    - `lambda_mult`: MMR 결과의 다양성 조절 (0-1 사이, 기본값: 0.5)\n",
    "    - `filter`: 문서 메타데이터 기반 필터링\n",
    "\n",
    "**반환값(return)**\n",
    "\n",
    "- `VectorStoreRetriever`: 초기화된 VectorStoreRetriever 객체\n",
    "\n",
    "**참고**\n",
    "\n",
    "- 다양한 검색 전략 구현 가능 (유사도, MMR, 임계값 기반)\n",
    "- MMR (Maximal Marginal Relevance) 알고리즘으로 검색 결과의 다양성 조절 가능\n",
    "- 메타데이터 필터링으로 특정 조건의 문서만 검색 가능\n",
    "- `tags` 매개변수를 통해 검색기에 태그 추가 가능\n",
    "\n",
    "**주의사항**\n",
    "\n",
    "- `search_type`과 `search_kwargs`의 적절한 조합 필요\n",
    "- MMR 사용 시 `fetch_k`와 `k` 값의 균형 조절 필요\n",
    "- `score_threshold` 설정 시 너무 높은 값은 검색 결과가 없을 수 있음\n",
    "- 필터 사용 시 데이터셋의 메타데이터 구조 정확히 파악 필요\n",
    "- `lambda_mult` 값이 0에 가까울수록 다양성이 높아지고, 1에 가까울수록 유사성이 높아짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f79a365f",
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1737384461972,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "f79a365f"
   },
   "outputs": [],
   "source": [
    "# 데이터베이스를 검색기로 사용하기 위해 retriever 변수에 할당\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d741849",
   "metadata": {
    "id": "6d741849"
   },
   "source": [
    "### Retriever의 invoke()\n",
    "\n",
    "`invoke` 메서드는 Retriever의 주요 진입점으로, 관련 문서를 검색하는 데 사용됩니다. 이 메서드는 동기적으로 Retriever를 호출하여 주어진 쿼리에 대한 관련 문서를 반환합니다.\n",
    "\n",
    "**매개변수(parameters)**\n",
    "\n",
    "- `input`: 검색 쿼리 문자열\n",
    "- `config`: Retriever 구성 (Optional[RunnableConfig])\n",
    "- `**kwargs`: Retriever에 전달할 추가 인자\n",
    "\n",
    "**반환값(return)**\n",
    "\n",
    "- `List[Document]`: 관련 문서 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8e39091",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 615,
     "status": "ok",
     "timestamp": 1737384469572,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "b8e39091",
    "outputId": "9519748e-ce8b-406f-dfca-bd9269052943"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "=========================================================\n",
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "=========================================================\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "=========================================================\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
      "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
      "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
      "\n",
      "Word2Vec\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# 관련 문서를 검색\n",
    "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\")\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab94c2c",
   "metadata": {
    "id": "6ab94c2c"
   },
   "source": [
    "### Max Marginal Relevance (MMR)\n",
    "\n",
    "`MMR(Maximal Marginal Relevance)` 방식은 쿼리에 대한 관련 항목을 검색할 때 검색된 문서의 **중복** 을 피하는 방법 중 하나입니다.\n",
    "\n",
    "단순히 가장 관련성 높은 항목들만을 검색하는 대신, MMR은 쿼리에 대한 **문서의 관련성** 과 이미 선택된 **문서들과의 차별성을 동시에 고려** 합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5c9cb",
   "metadata": {
    "id": "81f5c9cb"
   },
   "source": [
    "- `search_type` 매개변수를 `\"mmr\"` 로 설정하여 **MMR(Maximal Marginal Relevance)** 검색 알고리즘을 사용합니다.\n",
    "- `k`: 반환할 문서 수 (기본값: 4)\n",
    "- `fetch_k`: MMR 알고리즘에 전달할 문서 수 (기본값: 20)\n",
    "- `lambda_mult`: MMR 결과의 다양성 조절 (0~1, 기본값: 0.5, 0: 유사도 점수만 고려, 1: 다양성만 고려)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8144a926",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 537,
     "status": "ok",
     "timestamp": 1737384478682,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "8144a926",
    "outputId": "0eb16f40-cf7d-4732-80a5-e437610e5213"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "=========================================================\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# MMR(Maximal Marginal Relevance) 검색 유형을 지정\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"mmr\", search_kwargs={\"k\": 2, \"fetch_k\": 10, \"lambda_mult\": 0.6}\n",
    ")\n",
    "\n",
    "# 관련 문서를 검색합니다.\n",
    "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\")\n",
    "\n",
    "# 관련 문서를 검색\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "902babfe",
   "metadata": {
    "id": "902babfe"
   },
   "source": [
    "### 유사도 점수 임계값 검색(similarity_score_threshold)\n",
    "\n",
    "유사도 점수 임계값을 설정하고 해당 임계값 이상의 점수를 가진 문서만 반환하는 검색 방법을 설정할 수 있습니다.\n",
    "\n",
    "임계값을 적절히 설정함으로써 **관련성이 낮은 문서를 필터링** 하고, 질의와 **가장 유사한 문서만 선별** 할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476166f1",
   "metadata": {
    "id": "476166f1"
   },
   "source": [
    "- `search_type` 매개변수를 `\"similarity_score_threshold\"` 로 설정하여 유사도 점수 임계값을 기준으로 검색을 수행합니다.\n",
    "\n",
    "- `search_kwargs` 매개변수에 `{\"score_threshold\": 0.8}`를 전달하여 유사도 점수 임계값을 0.8로 설정합니다. 이는 검색 결과의 **유사도 점수가 0.8 이상인 문서만 반환됨** 을 의미합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6509f52",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 243,
     "status": "ok",
     "timestamp": 1737384483165,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "b6509f52",
    "outputId": "197b6cb2-6b75-4c78-b101-07eb332142ad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "retriever = db.as_retriever(\n",
    "    # 검색 유형을 \"similarity_score_threshold 으로 설정\n",
    "    search_type=\"similarity_score_threshold\",\n",
    "    # 임계값 설정\n",
    "    search_kwargs={\"score_threshold\": 0.8},\n",
    ")\n",
    "\n",
    "# 관련 문서를 검색\n",
    "for doc in retriever.invoke(\"Word2Vec 은 무엇인가요?\"):\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b16c14f",
   "metadata": {
    "id": "7b16c14f"
   },
   "source": [
    "### top_k 설정\n",
    "\n",
    "검색 시 사용할 `k` 와 같은 검색 키워드 인자(kwargs)를 지정할 수 있습니다.\n",
    "\n",
    "`k` 매개변수는 검색 결과에서 반환할 상위 결과의 개수를 나타냅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be90b9b8",
   "metadata": {
    "id": "be90b9b8"
   },
   "source": [
    "- `search_kwargs`에서 `k` 매개변수를 1로 설정하여 검색 결과로 반환할 문서의 수를 지정합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "081e0134",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1737384487608,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "081e0134",
    "outputId": "2f186279-58d9-4eb3-c30b-809f3b8ee5c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# k 설정\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1})\n",
    "\n",
    "# 관련 문서를 검색\n",
    "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\")\n",
    "\n",
    "# 관련 문서를 검색\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1168ca53",
   "metadata": {
    "id": "1168ca53"
   },
   "source": [
    "## 동적 설정(Configurable)\n",
    "\n",
    "- 검색 설정을 동적으로 조정하기 위해 `ConfigurableField` 를 사용합니다.\n",
    "- `ConfigurableField` 는 검색 매개변수의 고유 식별자, 이름, 설명을 설정하는 역할을 합니다.\n",
    "- 검색 설정을 조정하기 위해 `config` 매개변수를 사용하여 검색 설정을 지정합니다.\n",
    "- 검색 설정은 `config` 매개변수에 전달된 딕셔너리의 `configurable` 키에 저장됩니다.\n",
    "- 검색 설정은 검색 쿼리와 함께 전달되며, 검색 쿼리에 따라 동적으로 조정됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc25029f",
   "metadata": {
    "executionInfo": {
     "elapsed": 242,
     "status": "ok",
     "timestamp": 1737384494652,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "bc25029f"
   },
   "outputs": [],
   "source": [
    "from langchain_core.runnables import ConfigurableField\n",
    "\n",
    "# k 설정\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 1}).configurable_fields(\n",
    "    search_type=ConfigurableField(\n",
    "        id=\"search_type\",\n",
    "        name=\"Search Type\",\n",
    "        description=\"The search type to use\",\n",
    "    ),\n",
    "    search_kwargs=ConfigurableField(\n",
    "        # 검색 매개변수의 고유 식별자를 설정\n",
    "        id=\"search_kwargs\",\n",
    "        # 검색 매개변수의 이름을 설정\n",
    "        name=\"Search Kwargs\",\n",
    "        # 검색 매개변수에 대한 설명을 작성\n",
    "        description=\"The search kwargs to use\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721b750c",
   "metadata": {
    "id": "721b750c"
   },
   "source": [
    "아래는 동적 검색설정을 적용한 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2da4f32",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 500,
     "status": "ok",
     "timestamp": 1737384497814,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "f2da4f32",
    "outputId": "b9359e57-5e9c-49b4-8cda-226b6854f717"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\n",
      "예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\n",
      "연관키워드: 자연어 처리, 벡터화, 딥러닝\n",
      "\n",
      "Token\n",
      "=========================================================\n",
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "=========================================================\n",
      "Semantic Search\n",
      "\n",
      "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
      "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
      "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
      "\n",
      "Embedding\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# 검색 설정을 지정. Faiss 검색에서 k=3로 설정하여 가장 유사한 문서 3개를 반환\n",
    "config = {\"configurable\": {\"search_kwargs\": {\"k\": 3}}}\n",
    "\n",
    "# 관련 문서를 검색\n",
    "docs = retriever.invoke(\"임베딩(Embedding)은 무엇인가요?\", config=config)\n",
    "\n",
    "# 관련 문서를 검색\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48f53c12",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 270,
     "status": "ok",
     "timestamp": 1737384501140,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "48f53c12",
    "outputId": "bc280dbd-4f62-491f-e346-aa2eae51e105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# 검색 설정을 지정. score_threshold 0.8 이상의 점수를 가진 문서만 반환\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"search_type\": \"similarity_score_threshold\",\n",
    "        \"search_kwargs\": {\n",
    "            \"score_threshold\": 0.8,\n",
    "        },\n",
    "    }\n",
    "}\n",
    "\n",
    "# 관련 문서를 검색\n",
    "docs = retriever.invoke(\"Word2Vec 은 무엇인가요?\", config=config)\n",
    "\n",
    "# 관련 문서를 검색\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "951851c8",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 257,
     "status": "ok",
     "timestamp": 1737384503877,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "951851c8",
    "outputId": "917d61b2-9f3d-4019-ecaf-ccb255713279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\n",
      "예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\n",
      "연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\n",
      "LLM (Large Language Model)\n",
      "=========================================================\n",
      "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
      "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
      "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
      "\n",
      "Word2Vec\n",
      "=========================================================\n"
     ]
    }
   ],
   "source": [
    "# 검색 설정을 지정. mmr 검색 설정.\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"search_type\": \"mmr\",\n",
    "        \"search_kwargs\": {\"k\": 2, \"fetch_k\": 10, \"lambda_mult\": 0.6},\n",
    "    }\n",
    "}\n",
    "\n",
    "# 관련 문서를 검색\n",
    "docs = retriever.invoke(\"Word2Vec 은 무엇인가요?\", config=config)\n",
    "\n",
    "# 관련 문서를 검색\n",
    "for doc in docs:\n",
    "    print(doc.page_content)\n",
    "    print(\"=========================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbddf405",
   "metadata": {
    "id": "fbddf405"
   },
   "source": [
    "## Upstage 임베딩과 같이 Query & Passage embedding model 이 분리된 경우\n",
    "\n",
    "기본 retriever는 쿼리와 문서에 대해 동일한 임베딩 모델을 사용합니다.\n",
    "\n",
    "하지만 쿼리와 문서에 대해 서로 다른 임베딩 모델을 사용하는 경우가 있습니다.\n",
    "\n",
    "이러한 경우에는 쿼리 임베딩 모델을 사용하여 쿼리를 임베딩하고, 문서 임베딩 모델을 사용하여 문서를 임베딩합니다.\n",
    "\n",
    "이렇게 하면 쿼리와 문서에 대해 서로 다른 임베딩 모델을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "FUH1OE1YL-XW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5249,
     "status": "ok",
     "timestamp": 1737384535461,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "FUH1OE1YL-XW",
    "outputId": "4c004e51-103c-42a9-df33-3a1b1309f5f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/295.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m286.7/295.8 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m98.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m55.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qU langchain_upstage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f_W620ghMM9p",
   "metadata": {
    "executionInfo": {
     "elapsed": 245,
     "status": "ok",
     "timestamp": 1737384619983,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "f_W620ghMM9p"
   },
   "outputs": [],
   "source": [
    "os.environ[\"UPSTAGE_API_KEY\"] = \"up_17QFJKhVHmRAuHsirr1rC8nwJJ5ms\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6b51093b",
   "metadata": {
    "executionInfo": {
     "elapsed": 6212,
     "status": "ok",
     "timestamp": 1737384627449,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "6b51093b"
   },
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_upstage import UpstageEmbeddings\n",
    "\n",
    "# TextLoader를 사용하여 파일을 로드합니다.\n",
    "loader = TextLoader(\"./data/appendix-keywords.txt\")\n",
    "\n",
    "# 문서를 로드합니다.\n",
    "documents = loader.load()\n",
    "\n",
    "# 문자 기반으로 텍스트를 분할하는 CharacterTextSplitter를 생성합니다. 청크 크기는 300이고 청크 간 중복은 없습니다.\n",
    "text_splitter = CharacterTextSplitter(chunk_size=300, chunk_overlap=0)\n",
    "\n",
    "# 로드된 문서를 분할합니다.\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# Upstage 임베딩을 생성합니다. 문서용 모델을 사용합니다.\n",
    "doc_embedder = UpstageEmbeddings(model=\"solar-embedding-1-large-passage\")\n",
    "\n",
    "# 분할된 텍스트와 임베딩을 사용하여 FAISS 벡터 데이터베이스를 생성합니다.\n",
    "db = FAISS.from_documents(split_docs, doc_embedder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424f69e7",
   "metadata": {
    "id": "424f69e7"
   },
   "source": [
    "아래는 쿼리용 Upstage 임베딩을 생성하고, 쿼리 문장을 벡터로 변환하여 벡터 유사도 검색을 수행하는 예시입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa46e289",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1892,
     "status": "ok",
     "timestamp": 1737384636405,
     "user": {
      "displayName": "Chang Jun Lee",
      "userId": "07449846774346066151"
     },
     "user_tz": -540
    },
    "id": "aa46e289",
    "outputId": "6ed90bac-869a-417b-b4bd-f7aedc111714"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='75315fab-b8ff-451a-9ebe-32dc513fa6ff', metadata={'source': './data/appendix-keywords.txt'}, page_content='정의: 임베딩은 단어나 문장 같은 텍스트 데이터를 저차원의 연속적인 벡터로 변환하는 과정입니다. 이를 통해 컴퓨터가 텍스트를 이해하고 처리할 수 있게 합니다.\\n예시: \"사과\"라는 단어를 [0.65, -0.23, 0.17]과 같은 벡터로 표현합니다.\\n연관키워드: 자연어 처리, 벡터화, 딥러닝\\n\\nToken'),\n",
       " Document(id='6695bf5e-663c-49fd-9c38-fada6f1325e8', metadata={'source': './data/appendix-keywords.txt'}, page_content='정의: Word2Vec은 단어를 벡터 공간에 매핑하여 단어 간의 의미적 관계를 나타내는 자연어 처리 기술입니다. 이는 단어의 문맥적 유사성을 기반으로 벡터를 생성합니다.\\n예시: Word2Vec 모델에서 \"왕\"과 \"여왕\"은 서로 가까운 위치에 벡터로 표현됩니다.\\n연관키워드: 자연어 처리, 임베딩, 의미론적 유사성\\nLLM (Large Language Model)')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 쿼리용 Upstage 임베딩을 생성합니다. 쿼리용 모델을 사용합니다.\n",
    "query_embedder = UpstageEmbeddings(model=\"solar-embedding-1-large-query\")\n",
    "\n",
    "# 쿼리 문장을 벡터로 변환합니다.\n",
    "query_vector = query_embedder.embed_query(\"임베딩(Embedding)은 무엇인가요?\")\n",
    "\n",
    "# 벡터 유사도 검색을 수행하여 가장 유사한 2개의 문서를 반환합니다.\n",
    "db.similarity_search_by_vector(query_vector, k=2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "py-test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
