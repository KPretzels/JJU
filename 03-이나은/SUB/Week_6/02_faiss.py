# -*- coding: utf-8 -*-
"""02-FAISS.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ejYjh4RcdqWhRYREQ7z_pOdr3yzU4-zm
"""

!pip install -qU langchain_community langchain

from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size = 600, chunk_overlap = 0)

loader1 = TextLoader("/content/finance-keywords.txt")
loader2 = TextLoader("/content/nlp-keywords.txt")

split_doc1 = loader1.load_and_split(text_splitter)
split_doc2 = loader2.load_and_split(text_splitter)

!pip install -qU faiss-cpu langchain_openai

# 정리
import faiss
from langchain_community.vectorstores import FAISS
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain_openai import OpenAIEmbeddings

#임베딩 모델을 강제로 설정
embeddings = OpenAIEmbeddings(model = "text-embedding-3-large")

#임베딩 차원의 크기
dimension_size = len(embeddings.embed_query("hello world"))
print(dimension_size)

# FAISS 벡터 저장소 생성
db = FAISS(
    embedding_function=OpenAIEmbeddings(),
    index=faiss.IndexFlatL2(dimension_size),
    docstore=InMemoryDocstore(),
    index_to_docstore_id={},
)

# DB 생성
db = FAISS.from_documents(documents=split_doc1, embedding=OpenAIEmbeddings())

# 문서 저장소 ID 확인
db.index_to_docstore_id

# 저장된 문서의 ID: Document 확인
db.docstore._dict

# db2는 text를 불러와서 만든 데이터베이스
db2 = FAISS.from_texts(
    ['1번 문서 입니다.', '2번 문서 입니다.', '3번 문서 입니다.'],
    OpenAIEmbeddings(), # This line provides the embedding
    #embedding = OpenAIEmbeddings(), Remove this duplicate argument
    metadatas = [{"source" : "mydata.txt"}, {"source" : "mydata.txt"}, {"source" : "mydata.txt"}],
    ids = ["doc1", "doc2", "doc3"]
)

"""### 유사도 검사
`query (str)`: 유사한 문서를 찾기 위한 검색 쿼리 텍스트

`k (int)`: 반환할 문서 수. 기본값은 4

`filter (Optional[Union[Callable, Dict[str, Any]]])`: 메타데이터 필터링 함수 또는 딕셔너리. 기본값은 None

`fetch_k (int)`: 필터링 전에 가져올 문서 수. 기본값은 20

**kwargs**: 추가 키워드 인자
"""



# 유사도 검색
db.similarity_search("TF IDF 에 대하여 알려줘", k = 2)

# filter 사용
db.similarity_search(
    "TF IDF 에 대하여 알려줘", filter={"source": "data/nlp-keywords.txt"}, k=2
)

"""### 문서를 추가하는 방법"""

from langchain_core.documents import Document

db.add_documents(
    [
    Document(
        page_content = "안녕하세요! 이번엔 도큐먼트를 새로 추가해 볼게요.",
        metadata = {"socurce" : "mydata.txt"},

      )
    ],
    ids = ["new_doc4"]
)

db.similarity_search("안녕하세요!", k = 1) # 추가된 데이터 확인

"""### Text 추가"""

db.add_texts(
    ['5번 문서 입니다.', '6번 문서 입니다.'],
    metadatas = [{"source" : "mydata.txt"}, {"source" : "mydata.txt"}],
    ids = ["doc5", "doc6"]
)

# 추가된 데이터를 확인
db.index_to_docstore_id

"""### 데이터 삭제"""

# 삭제용 데이터를 추가
ids = db.add_texts(
    ["삭제용 데이터를 추가합니다.", "2번째 삭제용 데이터입니다."],
    metadatas=[{"source": "mydata.txt"}, {"source": "mydata.txt"}],
    ids=["delete_doc1", "delete_doc2"],
)

# 추가된 데이터를 확인
db.index_to_docstore_id

"""### Retriever(as_Retriever)"""

db = FAISS.from_documents(
    documents = split_doc1 + split_doc2,
    embedding = OpenAIEmbeddings()
)

retriever = db.as_retriever()
retriever.invoke("안녕하세요")

retriever = db.as_retriever(
    search_type = "mmr", # 유사도
    search_kwargs = {'k' : 6, 'lambda_mult' : 0.25, 'fetch_k' : 10}
)

retriever.invoke("안녕하세요")

# MMR 검색 수행, 상위 2개만 반환
retriever = db.as_retriever(search_type="mmr", search_kwargs={"k": 2, "fetch_k": 10})
retriever.invoke("Word2Vec 에 대하여 알려줘")

# 메타데이터 필터 적용
retriever = db.as_retriever(
    search_kwargs={"filter": {"source": "data/finance-keywords.txt"}, "k": 2}
)
retriever.invoke("ESG 에 대하여 알려줘")

retriever

