{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g0kDchS2lVvq"
   },
   "source": [
    "# UpstageLayoutAnalysisLoader\n",
    "UpstageLayoutAnalysisLoader 는 Upstage AI에서 제공하는 문서 분석 도구로, LangChain 프레임워크와 통합되어 사용할 수 있는 문서 로더입니다.\n",
    "\n",
    "주요 특징: - PDF, 이미지 등 다양한 형식의 문서에서 레이아웃 분석 수행 - 문서의 구조적 요소(제목, 단락, 표, 이미지 등)를 자동으로 인식 및 추출 - OCR 기능 지원 (선택적)\n",
    "\n",
    "UpstageLayoutAnalysisLoader는 단순한 텍스트 추출을 넘어 문서의 구조를 이해하고 요소 간 관계를 파악하여 보다 정확한 문서 분석을 가능하게 합니다.\n",
    "\n",
    "설치\n",
    "\n",
    "langchain-upstage 패키지를 설치 후 사용합니다.\n",
    "```\n",
    "pip install -U langchain-upstage\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1736150963564,
     "user": {
      "displayName": "배진우",
      "userId": "16041142222927328568"
     },
     "user_tz": -540
    },
    "id": "8wW_Uj0UlLym"
   },
   "outputs": [],
   "source": [
    "UPSTAGE_API_KEY = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 14927,
     "status": "ok",
     "timestamp": 1736150120908,
     "user": {
      "displayName": "배진우",
      "userId": "16041142222927328568"
     },
     "user_tz": -540
    },
    "id": "zxJNbOiGnDP5",
    "outputId": "c2386b8a-f94b-4537-98b5-409c208ea26f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-upstage\n",
      "  Downloading langchain_upstage-0.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: langchain-core<0.4,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain-upstage) (0.3.25)\n",
      "Collecting langchain-openai<0.3,>=0.2 (from langchain-upstage)\n",
      "  Downloading langchain_openai-0.2.14-py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting pypdf<5.0.0,>=4.2.0 (from langchain-upstage)\n",
      "  Downloading pypdf-4.3.1-py3-none-any.whl.metadata (7.4 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.10/dist-packages (from langchain-upstage) (2.32.3)\n",
      "Collecting tokenizers<0.20.0,>=0.19.1 (from langchain-upstage)\n",
      "  Downloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-upstage) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-upstage) (1.33)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-upstage) (0.2.3)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-upstage) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-upstage) (2.10.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-upstage) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4,>=0.3.0->langchain-upstage) (4.12.2)\n",
      "Collecting langchain-core<0.4,>=0.3.0 (from langchain-upstage)\n",
      "  Downloading langchain_core-0.3.29-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting openai<2.0.0,>=1.58.1 (from langchain-openai<0.3,>=0.2->langchain-upstage)\n",
      "  Downloading openai-1.59.3-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting tiktoken<1,>=0.7 (from langchain-openai<0.3,>=0.2->langchain-upstage)\n",
      "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.31.0->langchain-upstage) (2024.12.14)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers<0.20.0,>=0.19.1->langchain-upstage) (0.27.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->langchain-upstage) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->langchain-upstage) (2024.10.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.20.0,>=0.19.1->langchain-upstage) (4.67.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4,>=0.3.0->langchain-upstage) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-upstage) (0.28.1)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-upstage) (3.10.12)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-upstage) (1.0.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.3,>=0.2->langchain-upstage) (3.7.1)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.3,>=0.2->langchain-upstage) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.3,>=0.2->langchain-upstage) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.58.1->langchain-openai<0.3,>=0.2->langchain-upstage) (1.3.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-upstage) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4,>=0.3.0->langchain-upstage) (2.27.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai<0.3,>=0.2->langchain-upstage) (2024.11.6)\n",
      "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.58.1->langchain-openai<0.3,>=0.2->langchain-upstage) (1.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-upstage) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.125->langchain-core<0.4,>=0.3.0->langchain-upstage) (0.14.0)\n",
      "Downloading langchain_upstage-0.4.0-py3-none-any.whl (26 kB)\n",
      "Downloading langchain_openai-0.2.14-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_core-0.3.29-py3-none-any.whl (411 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m411.6/411.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pypdf-4.3.1-py3-none-any.whl (295 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.19.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading openai-1.59.3-py3-none-any.whl (454 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.8/454.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pypdf, tiktoken, tokenizers, openai, langchain-core, langchain-openai, langchain-upstage\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.21.0\n",
      "    Uninstalling tokenizers-0.21.0:\n",
      "      Successfully uninstalled tokenizers-0.21.0\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 1.57.4\n",
      "    Uninstalling openai-1.57.4:\n",
      "      Successfully uninstalled openai-1.57.4\n",
      "  Attempting uninstall: langchain-core\n",
      "    Found existing installation: langchain-core 0.3.25\n",
      "    Uninstalling langchain-core-0.3.25:\n",
      "      Successfully uninstalled langchain-core-0.3.25\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "transformers 4.47.1 requires tokenizers<0.22,>=0.21, but you have tokenizers 0.19.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed langchain-core-0.3.29 langchain-openai-0.2.14 langchain-upstage-0.4.0 openai-1.59.3 pypdf-4.3.1 tiktoken-0.8.0 tokenizers-0.19.1\n"
     ]
    }
   ],
   "source": [
    "! pip install -U langchain-upstage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22746,
     "status": "ok",
     "timestamp": 1736150170668,
     "user": {
      "displayName": "배진우",
      "userId": "16041142222927328568"
     },
     "user_tz": -540
    },
    "id": "FwcQnNeMnJRv",
    "outputId": "3000c6e0-be44-42b6-a74b-9c92e2dc19fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.7/34.7 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.4/203.4 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.2/137.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m40.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for kiwipiepy_model (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "! pip install -qU langchain-teddynote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1736150315242,
     "user": {
      "displayName": "배진우",
      "userId": "16041142222927328568"
     },
     "user_tz": -540
    },
    "id": "aNfaIzB1n1Uo"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"\"\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W__SYUXwoApg"
   },
   "source": [
    "[UpstageLayoutAnalysisLoader]\n",
    "주요 파라미터\n",
    "\n",
    "- file_path: 분석할 문서 경로\n",
    "- output_type: 출력 형식 [(기본값)'html', 'text']\n",
    "- split: 문서 분할 방식 ['none', 'element', 'page']\n",
    "- use_ocr=True: OCR 사용\n",
    "- exclude=[\"header\", \"footer\"]: 헤더, 푸터 제외"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21038,
     "status": "ok",
     "timestamp": 1736150612501,
     "user": {
      "displayName": "배진우",
      "userId": "16041142222927328568"
     },
     "user_tz": -540
    },
    "id": "jG-RUuw_oN_4",
    "outputId": "a686ad32-1e23-4438-b664-b2aa8ee05617"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='<h1 id='0' style='font-size:22px'>TransUNet: Transformers Make Strong<br>Encoders for Medical Image Segmentation</h1> <p id='1' data-category='paragraph' style='font-size:20px'>Jieneng Chen1, Yongyi Lu1, Qihang Yu1, Xiangde Luo2,<br>Ehsan Adeli3, Yan Wang4, Le Lu5, Alan L. Yuille1, and Yuyin Zhou3</p> <p id='2' data-category='equation'>$$^{2}\\mathrm{University~of~Electronice~and~Dniversity}}\\\\ {{\\mathrm{~^{2}~E l e c t r o n i c~S c i e n t e~E n a d~D r a t y e c h n o l o g y~G l i n a}}}\\\\ {{\\mathrm{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~$$</p> <p id='3' data-category='paragraph' style='font-size:14px'>Abstract. Medical image segmentation is an essential prerequisite for<br>developing healthcare systems, especially for disease diagnosis and treat-<br>ment planning. On various medical image segmentation tasks, the u-<br>shaped architecture, also known as U-Net, has become the de-facto stan-<br>dard and achieved tremendous success. However, due to the intrinsic<br>locality of convolution operations, U-Net generally demonstrates limi-<br>tations in explicitly modeling long-range dependency. Transformers, de-<br>signed for sequence-to-sequence prediction, have emerged as alternative<br>architectures with innate global self-attention mechanisms, but can re-<br>sult in limited localization abilities due to insufficient low-level details.<br>In this paper, we propose TransUNet, which merits both Transformers<br>and U-Net, as a strong alternative for medical image segmentation. On<br>one hand, the Transformer encodes tokenized image patches from a con-<br>volution neural network (CNN) feature map as the input sequence for<br>extracting global contexts. On the other hand, the decoder upsamples<br>the encoded features which are then combined with the high-resolution<br>CNN feature maps to enable precise localization.<br>We argue that Transformers can serve as strong encoders for medical im-<br>age segmentation tasks, with the combination of U-Net to enhance finer<br>details by recovering localized spatial information. TransUNet achieves<br>superior performances to various competing methods on different medical<br>applications including multi-organ segmentation and cardiac segmenta-<br>tion. Code and models are available at https://github. com/Beckschen/<br>TransUNet.</p> <p id='5' data-category='paragraph' style='font-size:18px'>1 Introduction</p> <p id='6' data-category='paragraph' style='font-size:16px'>Convolutional neural networks (CNNs), especially fully convolutional networks<br>(FCNs) [8], have become dominant in medical image segmentation. Among dif-<br>ferent variants, U-Net [12], which consists of a symmetric encoder-decoder net-<br>work with skip-connections to enhance detail retention, has become the de-facto<br>choice. Based on this line of approach, tremendous success has been achieved<br>in a wide range of medical applications such as cardiac segmentation from</p>' metadata={'page': 1}\n",
      "page_content='<p id='7' data-category='paragraph' style='font-size:14px'>2 J. Chen et al.</p> <p id='8' data-category='paragraph' style='font-size:20px'>magnetic resonance (MR) [16], organ segmentation from computed tomography<br>(CT) [7, 17, 19] and polyp segmentation [20] from colonoscopy videos.</p> <br><p id='9' data-category='paragraph' style='font-size:18px'>In spite of their exceptional representational power, CNN-based approaches<br>generally exhibit limitations for modeling explicit long-range relation, due to the<br>intrinsic locality of convolution operations. Therefore, these architectures gen-<br>erally yield weak performances especially for target structures that show large<br>inter-patient variation in terms of texture, shape and size. To overcome this lim-<br>itation, existing studies propose to establish self-attention mechanisms based on<br>CNN features [13, 15]. On the other hand, Transformers, designed for sequence-<br>to-sequence prediction, have emerged as alternative architectures which employ<br>dispense convolution operators entirely and solely rely on attention mechanisms<br>instead [14]. Unlike prior CNN-based methods, Transformers are not only power-<br>ful at modeling global contexts but also demonstrate superior transferability for<br>downstream tasks under large-scale pre-training. The success has been widely<br>witnessed in the field of machine translation and natural language processing<br>(NLP) [3,14]. More recently, attempts have also matched or even exceeded state-<br>of-the-art performances for various image recognition tasks [4, 18].</p> <p id='10' data-category='paragraph' style='font-size:16px'>In this paper, we present the first study which explores the potential of trans-<br>formers in the context of medical image segmentation. However, interestingly,<br>we found that a naive usage (i.e., use a transformer for encoding the tokenized<br>image patches, and then directly upsamples the hidden feature representations<br>into a dense output of full resolution) cannot produce a satisfactory result.</p> <br><p id='11' data-category='paragraph' style='font-size:16px'>This is due to that Transformers treat the input as 1D sequences and ex-<br>clusively focus on modeling the global context at all stages, therefore result in<br>low-resolution features which lack detailed localization information. And this<br>information cannot be effectively recovered by direct upsampling to the full res-<br>olution, therefore leads to a coarse segmentation outcome. On the other hand,<br>CNN architectures (e.g., U-Net [12]) provide an avenue for extracting low-level<br>visual cues which can well remedy such fine spatial details.</p> <br><p id='12' data-category='paragraph' style='font-size:16px'>To this end, we propose TransUNet, the first medical image segmentation<br>framework, which establishes self-attention mechanisms from the perspective of<br>sequence-to-sequence prediction. To compensate for the loss of feature resolu-<br>tion brought by Transformers, TransUNet employs a hybrid CNN-Transformer<br>architecture to leverage both detailed high-resolution spatial information from<br>CNN features and the global context encoded by Transformers. Inspired by the<br>u-shaped architectural design, the self-attentive feature encoded by Transformers<br>is then upsampled to be combined with different high-resolution CNN features<br>skipped from the encoding path, for enabling precise localization. We show that<br>such a design allows our framework to preserve the advantages of Transformers<br>and also benefit medical image segmentation. Empirical results suggest that our<br>Transformer-based architecture presents a better way to leverage self-attention<br>compared with previous CNN-based self-attention methods. Additionally, we ob-<br>serve that more intensive incorporation of low-level features generally leads to<br>a better segmentation accuracy. Extensive experiments demonstrate the superi-</p>' metadata={'page': 2}\n",
      "page_content='<p id='14' data-category='paragraph' style='font-size:14px'>ority of our method against other competing methods on various medical image<br>segmentation tasks.</p> <p id='15' data-category='paragraph' style='font-size:20px'>2 Related Works</p> <p id='16' data-category='paragraph' style='font-size:16px'>Combining CNNs with self-attention mechanisms. Various studies have<br>attempted to integrate self-attention mechanisms into CNNs by modeling global<br>interactions of all pixels based on the feature maps. For instance, Wang et al.<br>designed a non-local operator, which can be plugged into multiple intermediate<br>convolution layers [15]. Built upon the encoder-decoder u-shaped architecture,<br>Schlemper et al. [13] proposed additive attention gate modules which are inte-<br>grated into the skip-connections. Different from these approaches, we employ<br>Transformers for embedding global self-attention in our method.</p> <br><p id='17' data-category='paragraph' style='font-size:16px'>Transformers. Transformers were first proposed by [14] for machine translation<br>and established state-of-the-arts in many NLP tasks. To make Transformers also<br>applicable for computer vision tasks, several modifications have been made. For<br>instance, Parmar et al. [11] applied the self-attention only in local neighborhoods<br>for each query pixel instead of globally. Child et al. [1] proposed Sparse Trans-<br>formers, which employ scalable approximations to global self-attention. Recently,<br>Vision Transformer (ViT) [] achieved state-of-the-art on ImageNet classification<br>by directly applying Transformers with global self-attention to full-sized images.<br>To the best of our knowledge, the proposed TransUNet is the first Transformer-<br>based medical image segmentation framework, which builds upon the highly<br>successful ViT.</p> <h1 id='18' style='font-size:20px'>3 Method</h1> <p id='19' data-category='paragraph' style='font-size:14px'>Given an image x E RHxWxC<br>with an spatial resolution of H x W and C num-<br>ber of channels. Our goal is to predict the corresponding pixel-wise labelmap<br>with size H x W. The most common way is to directly train a CNN (e.g., U-<br>Net) to first encode images into high-level feature representations, which are<br>then decoded back to the full spatial resolution. Unlike existing approaches, our<br>method introduces self-attention mechanisms into the encoder design via the us-<br>age of Transformers. We will first introduce how to directly apply transformer for<br>encoding feature representations from decomposed image patches in Section 3.1.<br>Then, the overall framework of TransUNet will be elaborated in Section 3.2.</p> <p id='20' data-category='paragraph' style='font-size:14px'>3.1 Transformer as Encoder</p> <p id='21' data-category='paragraph' style='font-size:14px'>Image Sequentialization. Following [4], we first perform tokenization by re-<br>shaping the input x into a sequence of flattened 2D patches {xi E RP2 �Pi =<br>HW of image<br>1,... N}, where each patch is of size P x P and N = is the number<br>p2<br>patches (i.e., the input sequence length).</p>' metadata={'page': 3}\n"
     ]
    }
   ],
   "source": [
    "from langchain_upstage import UpstageLayoutAnalysisLoader\n",
    "\n",
    "# 파일 경로\n",
    "file_path = \"/content/TransUNet_10p.pdf\"\n",
    "\n",
    "# 문서 로더 설정\n",
    "loader = UpstageLayoutAnalysisLoader(\n",
    "    file_path,\n",
    "    output_type=\"html\",\n",
    "    split=\"page\",\n",
    "    use_ocr=True,\n",
    "    exclude=[\"header\", \"footer\"],\n",
    "    api_key=UPSTAGE_API_KEY\n",
    ")\n",
    "\n",
    "# 문서 로드\n",
    "docs = loader.load()\n",
    "\n",
    "# 결과 출력\n",
    "for doc in docs[:3]:\n",
    "    print(doc)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOoY8kvRTsOBlY1HYclyroC",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
