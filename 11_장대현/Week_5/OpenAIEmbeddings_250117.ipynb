{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "| **Model**                | **Pages per Dollar** | **Performance on MTEB Eval** | **Max Input** |\n",
        "|--------------------------|----------------------|------------------------------|---------------|\n",
        "| text-embedding-3-small    | 62,500               | 62.3%                        | 8191          |\n",
        "| text-embedding-3-large    | 9,615                | 64.6%                        | 8191          |\n",
        "| text-embedding-ada-002    | 12,500               | 61.0%                        | 8191          |\n"
      ],
      "metadata": {
        "id": "vp63vqe6xfGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu_bxG2WxS4F",
        "outputId": "275b42fb-d301-4da7-8cd5-aa71f61f91d4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.2/54.2 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m0.9/1.2 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
      ],
      "metadata": {
        "id": "4HH_rIxfxS6k"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"우주의 기원은 어디인가요?\""
      ],
      "metadata": {
        "id": "QFEEEDB-xS9U"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Query Embedding"
      ],
      "metadata": {
        "id": "GkHNC3BjyWu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query_result = embeddings.embed_query(text)"
      ],
      "metadata": {
        "id": "OCmSWp_7ybBE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query_result[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FI59ZlnPy2PU",
        "outputId": "945813e5-42b0-4917-d37c-ce1d896301cb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0487622432410717,\n",
              " -0.005244841333478689,\n",
              " 0.03377220407128334,\n",
              " 0.027035608887672424,\n",
              " -0.005228111054748297]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Document Embedding"
      ],
      "metadata": {
        "id": "R1RCTkMaz9zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_2 = \"심해는 해양의 깊은 곳으로, 빛이 거의 닿지 않는 200m 이하의 영역입니다. 고압, 저온 환경에서 독특한 생물이 서식합니다.\"\n",
        "\n",
        "doc_result = embeddings.embed_documents(\n",
        "    [text_2, text_2, text_2, text_2]\n",
        ")   # 텍스트를 임베딩하여 문서 벡터를 생성"
      ],
      "metadata": {
        "id": "9KNh3-sNz4GL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc_result[0][:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OztHOpvz32B",
        "outputId": "5f064a85-51b9-4f81-99dc-01ff35ee250c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0064157177694141865,\n",
              " -9.356773080071434e-05,\n",
              " 0.006743709556758404,\n",
              " 0.03315698727965355,\n",
              " 0.026159828528761864]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 차원 지정"
      ],
      "metadata": {
        "id": "ViN69Pk7HX8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI의 \"text-embedding-3-small\" 모델을 사용하여 1024차원의 임베딩을 생성하는 객체를 초기화\n",
        "scaled_embeddings = OpenAIEmbeddings(model = \"text-embedding-3-small\", dimensions = 1024)"
      ],
      "metadata": {
        "id": "d14_k5AJHcO_"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(scaled_embeddings.embed_documents([text])[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNF5qginIDi4",
        "outputId": "8172fa73-6759-4624-b117-e9a850272f7c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Similarity Calc."
      ],
      "metadata": {
        "id": "CHip6NjkIw0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"안녕하세요? 반갑습니다.\"\n",
        "sentence2 = \"안녕하세요? 반갑습니다!\"\n",
        "sentence3 = \"안녕하세요? 만나서 반가워요.\"\n",
        "sentence4 = \"Hi, nice to meet you.\"\n",
        "sentence5 = \"I like to eat apples.\""
      ],
      "metadata": {
        "id": "-hzYVdsNIQNB"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "sentences = [sentence1, sentence2, sentence3, sentence4, sentence5]"
      ],
      "metadata": {
        "id": "8DNkn2INJEOq"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedded_sentences = scaled_embeddings.embed_documents(sentences)"
      ],
      "metadata": {
        "id": "2VGnmmQGJEL7"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(embedded_sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0z60YgucLAcg",
        "outputId": "a0efedfd-3e17-4603-f4f0-231ff0ea26fa"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1024"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def similarity(a, b):\n",
        "    return cosine_similarity([a], [b])[0][0]"
      ],
      "metadata": {
        "id": "VpM4bcbCLH_B"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sentence in enumerate(embedded_sentences):\n",
        "    for j, other_sentence in enumerate(embedded_sentences):\n",
        "        if i < j:\n",
        "            print(\n",
        "                f\"[유사도 {similarity(sentence, other_sentence):.4f}] {sentences[i]} \\t <=====> \\t {sentences[j]}\"\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "poZCu9ChLqkA",
        "outputId": "3480af91-5145-40a2-8e7f-e4cc114401f8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[유사도 0.9644] 안녕하세요? 반갑습니다. \t <=====> \t 안녕하세요? 반갑습니다!\n",
            "[유사도 0.8378] 안녕하세요? 반갑습니다. \t <=====> \t 안녕하세요? 만나서 반가워요.\n",
            "[유사도 0.5044] 안녕하세요? 반갑습니다. \t <=====> \t Hi, nice to meet you.\n",
            "[유사도 0.1363] 안녕하세요? 반갑습니다. \t <=====> \t I like to eat apples.\n",
            "[유사도 0.8144] 안녕하세요? 반갑습니다! \t <=====> \t 안녕하세요? 만나서 반가워요.\n",
            "[유사도 0.4792] 안녕하세요? 반갑습니다! \t <=====> \t Hi, nice to meet you.\n",
            "[유사도 0.1318] 안녕하세요? 반갑습니다! \t <=====> \t I like to eat apples.\n",
            "[유사도 0.5128] 안녕하세요? 만나서 반가워요. \t <=====> \t Hi, nice to meet you.\n",
            "[유사도 0.1409] 안녕하세요? 만나서 반가워요. \t <=====> \t I like to eat apples.\n",
            "[유사도 0.2249] Hi, nice to meet you. \t <=====> \t I like to eat apples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 언어 간 임베딩 차이 줄이기"
      ],
      "metadata": {
        "id": "aZOq_LpRO3y7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# GPT-4o 모델 설정\n",
        "chat_model = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# GPT-4o를 사용해 문장을 영어로 번역하는 함수\n",
        "def translate_to_english(sentence):\n",
        "    response = chat_model.predict_messages(\n",
        "        [\n",
        "            {\"role\": \"system\", \"content\": \"Translate the given text to English.\"},\n",
        "            {\"role\": \"user\", \"content\": sentence},\n",
        "        ]\n",
        "    )\n",
        "    return response.content\n",
        "\n",
        "# 문장 리스트\n",
        "sentences = [\n",
        "    \"안녕하세요? 반갑습니다.\",\n",
        "    \"안녕하세요? 반갑습니다!\",\n",
        "    \"안녕하세요? 만나서 반가워요.\",\n",
        "    \"Hi, nice to meet you.\",\n",
        "    \"I like to eat apples.\",\n",
        "]\n",
        "\n",
        "# 문장을 영어로 번역\n",
        "translated_sentences = [translate_to_english(sentence) for sentence in sentences]\n",
        "\n",
        "# scaled_embeddings를 적절히 설정하세요\n",
        "embedded_sentences = scaled_embeddings.embed_documents(translated_sentences)\n",
        "\n",
        "# 유사도 계산 함수\n",
        "def similarity(a, b):\n",
        "    return cosine_similarity([a], [b])[0][0]\n",
        "\n",
        "# 유사도 출력\n",
        "for i, sentence in enumerate(embedded_sentences):\n",
        "    for j, other_sentence in enumerate(embedded_sentences):\n",
        "        if i < j:\n",
        "            print(\n",
        "                f\"[유사도 {similarity(sentence, other_sentence):.4f}] {sentences[i]} \\t <=====> \\t {sentences[j]}\"\n",
        "            )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxhFfEnaLtCR",
        "outputId": "f8c399be-76ac-46c0-c557-b1b7d830d2b8"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-52-69b360b97c52>:9: LangChainDeprecationWarning: The method `BaseChatModel.predict_messages` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = chat_model.predict_messages(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[유사도 0.9459] 안녕하세요? 반갑습니다. \t <=====> \t 안녕하세요? 반갑습니다!\n",
            "[유사도 1.0000] 안녕하세요? 반갑습니다. \t <=====> \t 안녕하세요? 만나서 반가워요.\n",
            "[유사도 0.8406] 안녕하세요? 반갑습니다. \t <=====> \t Hi, nice to meet you.\n",
            "[유사도 0.2018] 안녕하세요? 반갑습니다. \t <=====> \t I like to eat apples.\n",
            "[유사도 0.9459] 안녕하세요? 반갑습니다! \t <=====> \t 안녕하세요? 만나서 반가워요.\n",
            "[유사도 0.7976] 안녕하세요? 반갑습니다! \t <=====> \t Hi, nice to meet you.\n",
            "[유사도 0.1878] 안녕하세요? 반갑습니다! \t <=====> \t I like to eat apples.\n",
            "[유사도 0.8406] 안녕하세요? 만나서 반가워요. \t <=====> \t Hi, nice to meet you.\n",
            "[유사도 0.2018] 안녕하세요? 만나서 반가워요. \t <=====> \t I like to eat apples.\n",
            "[유사도 0.2249] Hi, nice to meet you. \t <=====> \t I like to eat apples.\n"
          ]
        }
      ]
    }
  ]
}