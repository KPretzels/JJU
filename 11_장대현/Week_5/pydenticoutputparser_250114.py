# -*- coding: utf-8 -*-
"""PydenticOutputParser_250114.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t2VPE4cd7dSjMqMay3a69qjZdOLs5NC5
"""

!pip install -qU langchain_openai

from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import PydanticOutputParser
from pydantic import BaseModel, Field

llm = ChatOpenAI(temperature=0, model_name = "gpt-4o")

email_conversation = """
From: John (John@bikecorporation.me)
To: Kim (Kim@teddyinternational.me)
Subject: “ZENESIS” bike distribution cooperation and meeting schedule proposal
Dear Mr. Kim,

I am John, Senior Executive Director at Bike Corporation. I recently learned about your new bicycle model, "ZENESIS," through your press release. Bike Corporation is a company that leads innovation and quality in the field of bicycle manufacturing and distribution, with long-time experience and expertise in this field.

We would like to request a detailed brochure for the ZENESIS model. In particular, we need information on technical specifications, battery performance, and design aspects. This information will help us further refine our proposed distribution strategy and marketing plan.

Additionally, to discuss the possibilities for collaboration in more detail, I propose a meeting next Tuesday, January 15th, at 10:00 AM. Would it be possible to meet at your office to have this discussion?

Thank you.

Best regards,
John
Senior Executive Director
Bike Corporation
"""

"""# Output Parser를 사용하지 않는 경우"""

from itertools import chain
from langchain_core.prompts import PromptTemplate
from langchain_core.messages import AIMessageChunk
from langchain_core.output_parsers import StrOutputParser

prompt = PromptTemplate.from_template(
    """Please extract the important parts of the following email.

    {email_conversation}"""
)

llm = ChatOpenAI(temperature=0, model_name = "gpt-4o")

chain = prompt | llm # | StrOutputParser()

answer = chain.stream({"email_conversation": email_conversation})

def stream_response(response, return_output = False):
  """
  Streams the response from the AI model, processing and printing each chunk.

  This function iterates over each item in the 'response' iterable. If an item is an instance of AIMessageChunk, it extracts and prints the content.
  If the item is a string, it prints the string directly.
  Optionally, the function can return the concatenated string of all response chunks.

  Args:
   - response (iterable): An iterable of response chunks, which can be AIMessageChunk objects or strings.
   - return_output (bool, optional): If True, the function returns the concatenated response string. The default is False.

  Returns:
   - str: If `return_output` is True, the concatenated response string. Otherwise, nothing is returned.
  """
  answer = ""
  for token in response:
    if isinstance(token, AIMessageChunk):
      answer += token.content
      print(token.content, end = "", flush = True)
    elif isinstance(token, str):
      answer += token
      print(token, end = "", flush = True)
  if return_output:
    return answer

output = stream_response(answer, return_output = True)

answer = chain.invoke({"email_conversation": email_conversation})
print(answer)

"""# Output Parser를 사용하는 경우"""

class EmailSummary(BaseModel):
  person: str = Field(description = "The sender of the email")
  email: str = Field(description = "The email address of the sender")
  subject: str = Field(description = "The subject of the email")
  summary: str = Field(description = "A summary of the email content")
  date: str = Field(
      description = "The meeting date and time mentioned in the email content"
  )

# Create PydanticOutputParser
parser = PydanticOutputParser(pydantic_object = EmailSummary)

print(parser.get_format_instructions())

"""# Prompt 제작"""

prompt = PromptTemplate.from_template(
    """
You are a helpful assistant.

Question:
{question}

EMAIL COVERSATION:
{email_conversation}

FORMAT:
{format}
"""
)

prompt = prompt.partial(format = parser.get_format_instructions())

prompt

chain = prompt | llm

response = chain.stream(
    {
        "email_conversation": email_conversation,
        "question": "이메일을 요약해."
    }
)

output = stream_response(response, return_output = True)

"""# With parser"""

chain = prompt | llm | parser

response = chain.invoke(
    {
        "email_conversation": email_conversation,
        "question": "Extract the main content of the email."
    }
)

# The results are output in the form of an EmailSummary object.
print(response)

response.person