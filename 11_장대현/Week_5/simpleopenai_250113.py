# -*- coding: utf-8 -*-
"""SimpleOpenAI_250113.ipynb의 사본

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zkrEVttj9oDEtGcrc_LGKpl-uK7hCPdE
"""

!pip install -qU langchain_openai

from langchain_openai import ChatOpenAI

llm = ChatOpenAI(
    model = "gpt-4o",
    temperature = 0,
    max_tokens = None,
    timeout = None,
    max_retries = 2,
    # base_url = "...",
    # organization = "...",
    # other params...
)

llm

prompt = "양자역학 이론은?"

result = llm.invoke(prompt)

result

result.content

"""# 2. PromptTemplate_Basic"""

prompt_2 = [
    ("system", "너는 제시된 국가의 수도가 어디인지 알려주는 어시스턴트야."),
    ("human", "미합중국"),
]

prompt_2

response_2 = llm.invoke(prompt_2)

response_2

"""# 3_1 PromptTemplate(1)"""

from langchain_core.prompts import PromptTemplate

template = "{topic}의 개념은?"

prompt_3 = PromptTemplate.from_template(template)

print(prompt_3)

prompt_3 = prompt_3.format(topic = "양자역학")

print(prompt_3)

result_3 = llm.invoke(prompt_3)

result_3

"""# 3_2 PromptTemplate(2)"""

from langchain_core.prompts import PromptTemplate

template = "{topic_1}과 {topic_2}의 차이점은?"

prompt_4 = PromptTemplate(
    template = template,
    input_variables = ["topic"],
)

print(prompt_4)

prompt_4.format(topic = "양자역학")

from langchain_core.prompts import PromptTemplate

template = "{topic_1}과 {topic_2}의 차이점은?"

prompt_5 = PromptTemplate(
    template = template,
    input_variables = ["topic"],
    partial_variables= {"topic_2": "인공지능"}
)

print(prompt_5)

prompt_5.format(topic_1 = "양자역학")

chain = prompt_5 | llm

chain.invoke({"topic_1": "양자역학"})

"""# Partial Variable 사용 예"""

from functools import partial
from datetime import datetime
from langchain_core.prompts import PromptTemplate

prompt_6 = PromptTemplate(
    template = "오늘의 날짜는 {today} 입니다. 오늘이 생일인 유명인 {n}명을 나열해 주세요. 생년월일을 표기해주세요.",
    input_variables = {"n"},
    partial_variables = {"today": datetime.now().strftime("%Y %B %d")}
)

prompt_6.format(n = 3)

chain = prompt_6 | llm

response_6 = chain.invoke({"n": 3}).content

print(response_6)

"""# PromptTemplate YAML 데이터 읽어오기"""

from langchain_core.prompts import load_prompt

prompt_yaml_1 = load_prompt("/content/data/fruit_color.yaml")

prompt_yaml_1

prompt_yaml_1.format(fruit = "바나나")

print(prompt_yaml_1)

chain = prompt_yaml_1 | llm

chain.invoke({"fruit": "바나나"}).content