{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-teddynote"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK3jZjDEAt9g",
        "outputId": "08a497c6-1d38-42d1-a49b-10e0d846eae6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.7/34.7 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.1/51.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.6/208.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.3/145.3 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.6/114.6 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for kiwipiepy_model (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 문서를 예쁘게 출력하기 위한 도우미 함수\n",
        "def pretty_print_docs(docs):\n",
        "    print(\n",
        "        f\"\\n{'-' * 100}\\n\".join(\n",
        "            [f\"문서 {i+1}:\\n\\n\" + d.page_content for i, d in enumerate(docs)]\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "0WojjhrPAzb4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-community langchain_openai faiss-cpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN57OHukBTp6",
        "outputId": "7485f96f-f50b-4b25-efc2-fcb2b0ac5fc4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.2/412.2 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.8/50.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "# TextLoader를 사용하여 \"appendix-keywords.txt\" 파일에서 문서를 로드\n",
        "loader = TextLoader(\"/content/appendix-keywords.txt\")\n",
        "\n",
        "# CharacterTextSplitter를 사용하여 문서를 청크 크기 300자와 청크 간 중복 0으로 분할\n",
        "text_splitter = CharacterTextSplitter(chunk_size = 300, chunk_overlap = 0)\n",
        "texts = loader.load_and_split(text_splitter)\n",
        "\n",
        "# OpenAIEmbeddings를 사용하여 FAISS 벡터 저장소를 생성하고 검색기로 변환\n",
        "retriever = FAISS.from_documents(texts, OpenAIEmbeddings()).as_retriever()\n",
        "\n",
        "# 쿼리에 질문을 정의하고 관련 문서를 검색\n",
        "docs = retriever.invoke(\"Semantic Search 에 대해서 알려줘.\")\n",
        "\n",
        "# 검색된 문서를 예쁘게 출력\n",
        "pretty_print_docs(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aj6NJ-T3BjYc",
        "outputId": "8bdbb396-26e5-44a3-fc3b-dee20b6f3d16"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서 1:\n",
            "\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
            "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
            "\n",
            "Embedding\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 2:\n",
            "\n",
            "정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\n",
            "예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\n",
            "연관키워드: 검색 엔진, 데이터 검색, 정보 검색\n",
            "\n",
            "Page Rank\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 3:\n",
            "\n",
            "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
            "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
            "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
            "\n",
            "Word2Vec\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 4:\n",
            "\n",
            "정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\n",
            "예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\n",
            "연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\n",
            "\n",
            "데이터 마이닝\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_teddynote.document_compressors import LLMChainExtractor\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "\n",
        "# from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(temperature = 0, model = \"gpt-4o-mini\")  # OpenAI 언어 모델 초기화\n",
        "\n",
        "# LLM을 사용하여 문서 압축기 생성\n",
        "compressor = LLMChainExtractor.from_llm(llm)\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    # 문서 압축기와 리트리버를 사용하여 컨텍스트 압축 리트리버 생성\n",
        "    base_compressor=compressor,\n",
        "    base_retriever=retriever,\n",
        ")\n",
        "\n",
        "pretty_print_docs(retriever.invoke(\"Semantic Search 에 대해서 알려줘.\"))\n",
        "\n",
        "print(\"=========================================================\")\n",
        "print(\"============== LLMChainExtractor 적용 후 ==================\")\n",
        "\n",
        "compressed_docs = (\n",
        "    compression_retriever.invoke(  # 컨텍스트 압축 리트리버를 사용하여 관련 문서 검색\n",
        "        \"Semantic Search 에 대해서 알려줘.\"\n",
        "    )\n",
        ")\n",
        "pretty_print_docs(compressed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwkegTjFCuoV",
        "outputId": "bea56b6c-0dd2-407e-d28d-27fa2922486c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서 1:\n",
            "\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
            "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
            "\n",
            "Embedding\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 2:\n",
            "\n",
            "정의: 키워드 검색은 사용자가 입력한 키워드를 기반으로 정보를 찾는 과정입니다. 이는 대부분의 검색 엔진과 데이터베이스 시스템에서 기본적인 검색 방식으로 사용됩니다.\n",
            "예시: 사용자가 \"커피숍 서울\"이라고 검색하면, 관련된 커피숍 목록을 반환합니다.\n",
            "연관키워드: 검색 엔진, 데이터 검색, 정보 검색\n",
            "\n",
            "Page Rank\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 3:\n",
            "\n",
            "정의: 크롤링은 자동화된 방식으로 웹 페이지를 방문하여 데이터를 수집하는 과정입니다. 이는 검색 엔진 최적화나 데이터 분석에 자주 사용됩니다.\n",
            "예시: 구글 검색 엔진이 인터넷 상의 웹사이트를 방문하여 콘텐츠를 수집하고 인덱싱하는 것이 크롤링입니다.\n",
            "연관키워드: 데이터 수집, 웹 스크래핑, 검색 엔진\n",
            "\n",
            "Word2Vec\n",
            "----------------------------------------------------------------------------------------------------\n",
            "문서 4:\n",
            "\n",
            "정의: 페이지 랭크는 웹 페이지의 중요도를 평가하는 알고리즘으로, 주로 검색 엔진 결과의 순위를 결정하는 데 사용됩니다. 이는 웹 페이지 간의 링크 구조를 분석하여 평가합니다.\n",
            "예시: 구글 검색 엔진은 페이지 랭크 알고리즘을 사용하여 검색 결과의 순위를 정합니다.\n",
            "연관키워드: 검색 엔진 최적화, 웹 분석, 링크 분석\n",
            "\n",
            "데이터 마이닝\n",
            "=========================================================\n",
            "============== LLMChainExtractor 적용 후 ==================\n",
            "문서 1:\n",
            "\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Filtering"
      ],
      "metadata": {
        "id": "LV0-k8ATEYV7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filtering_1"
      ],
      "metadata": {
        "id": "sVEo403vFFwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_teddynote.document_compressors import LLMChainFilter\n",
        "\n",
        "# LLM을 사용하여 LLMChainFilter 객체를 생성\n",
        "_filter = LLMChainFilter.from_llm(llm)\n",
        "\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    # LLMChainFilter와 retriever를 사용하여 ContextualCompressionRetriever 객체를 생성\n",
        "    base_compressor = _filter,\n",
        "    base_retriever = retriever,\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.invoke(\n",
        "    # 쿼리\n",
        "    \"Semantic Search 에 대해서 알려줘.\"\n",
        ")\n",
        "pretty_print_docs(compressed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzBBH6_7Dpvj",
        "outputId": "522aedcf-f218-44c4-d1ef-05e5a904bcd0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서 1:\n",
            "\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
            "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
            "\n",
            "Embedding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filtering_2"
      ],
      "metadata": {
        "id": "nFuvGTjhFJ5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "\n",
        "# 유사도 임계값이 0.86인 EmbeddingsFilter 객체를 생성\n",
        "embeddings_filter = EmbeddingsFilter(embeddings = embeddings, similarity_threshold = 0.86)\n",
        "\n",
        "# 기본 압축기로 embeddings_filter를, 기본 검색기로 retriever를 사용하여 ContextualCompressionRetriever 객체를 생성\n",
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    base_compressor=embeddings_filter, base_retriever = retriever\n",
        ")\n",
        "\n",
        "# ContextualCompressionRetriever 객체를 사용하여 관련 문서를 검색\n",
        "compressed_docs = compression_retriever.invoke(\n",
        "    # 쿼리\n",
        "    \"Semantic Search 에 대해서 알려줘.\"\n",
        ")\n",
        "pretty_print_docs(compressed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON4QF3FTEqki",
        "outputId": "e0badaa0-5bb4-4eb7-97a2-9fec8d108623"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서 1:\n",
            "\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n",
            "연관키워드: 자연어 처리, 검색 알고리즘, 데이터 마이닝\n",
            "\n",
            "Embedding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline"
      ],
      "metadata": {
        "id": "a19yoSKNGLIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.retrievers.document_compressors import DocumentCompressorPipeline\n",
        "from langchain_community.document_transformers import EmbeddingsRedundantFilter\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "\n",
        "# 문자 기반 텍스트 분할기를 생성하고, 청크 크기를 300으로, 청크 간 중복을 0으로 설정\n",
        "splitter = CharacterTextSplitter(chunk_size = 300, chunk_overlap = 0)\n",
        "\n",
        "# 임베딩을 사용하여 중복 필터를 생성\n",
        "redundant_filter = EmbeddingsRedundantFilter(embeddings = embeddings)\n",
        "\n",
        "# 임베딩을 사용하여 관련성 필터를 생성하고, 유사도 임계값을 0.86으로 설정\n",
        "relevant_filter = EmbeddingsFilter(embeddings = embeddings, similarity_threshold = 0.86)\n",
        "\n",
        "pipeline_compressor = DocumentCompressorPipeline(\n",
        "    # 문서 압축 파이프라인을 생성하고, 분할기, 중복 필터, 관련성 필터, LLMChainExtractor를 변환기로 설정\n",
        "    transformers=[\n",
        "        splitter,\n",
        "        redundant_filter,\n",
        "        relevant_filter,\n",
        "        LLMChainExtractor.from_llm(llm),\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "1h3qCh3yGJ7r"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "compression_retriever = ContextualCompressionRetriever(\n",
        "    # 기본 압축기로 pipeline_compressor를 사용하고, 기본 검색기로 retriever를 사용하여 ContextualCompressionRetriever를 초기화\n",
        "    base_compressor = pipeline_compressor,\n",
        "    base_retriever = retriever,\n",
        ")\n",
        "\n",
        "compressed_docs = compression_retriever.invoke(\n",
        "    # 쿼리\n",
        "    \"Semantic Search 에 대해서 알려줘.\"\n",
        ")\n",
        "pretty_print_docs(compressed_docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WPBDQseZGZ7W",
        "outputId": "a6780bde-6913-4ee9-afed-91d901d2280a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "문서 1:\n",
            "\n",
            "Semantic Search\n",
            "\n",
            "정의: 의미론적 검색은 사용자의 질의를 단순한 키워드 매칭을 넘어서 그 의미를 파악하여 관련된 결과를 반환하는 검색 방식입니다.\n",
            "예시: 사용자가 \"태양계 행성\"이라고 검색하면, \"목성\", \"화성\" 등과 같이 관련된 행성에 대한 정보를 반환합니다.\n"
          ]
        }
      ]
    }
  ]
}