{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZpHww7fb_Wx"
   },
   "outputs": [],
   "source": [
    "pip install langchain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZwDECEaEcAYJ"
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "# 파일에서 텍스트를 읽고 주제별로 분할하는 함수\n",
    "def split_text_into_chunks(file_path, chunk_size=500, chunk_overlap=50):\n",
    "    \"\"\"\n",
    "    주어진 파일에서 텍스트를 읽고, TEXTSPLITTER를 사용하여 주제별로 나눕니다.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): 읽어올 파일 경로\n",
    "        chunk_size (int): 각 블록의 최대 크기\n",
    "        chunk_overlap (int): 인접한 블록 간의 중복되는 부분 크기\n",
    "\n",
    "    Returns:\n",
    "        list: 분할된 텍스트 블록들의 리스트\n",
    "    \"\"\"\n",
    "    # 파일 읽기\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        file_content = f.read()\n",
    "\n",
    "    # 텍스트 분할기 설정\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    split_text = text_splitter.split_text(file_content)\n",
    "\n",
    "    return split_text\n",
    "\n",
    "# 파일 경로 설정\n",
    "file_path = \"/content/appendix-keywords.txt\"\n",
    "\n",
    "# 텍스트 분할\n",
    "chunks = split_text_into_chunks(file_path, chunk_size=500, chunk_overlap=50)\n",
    "\n",
    "# 결과 출력\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(chunk)\n",
    "    print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uejvp5aG44ig"
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# API 키를 직접 입력\n",
    "api_key = \"\"\n",
    "genai.configure(api_key=api_key)\n",
    "\n",
    "# 모델 설정\n",
    "generation_config = {\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 0.95,\n",
    "  \"top_k\": 40,\n",
    "  \"max_output_tokens\": 8192,\n",
    "  \"response_mime_type\": \"text/plain\",  # 필요에 따라 수정 가능\n",
    "}\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "  model_name=\"gemini-2.0-flash-exp\",\n",
    "  generation_config=generation_config,\n",
    ")\n",
    "\n",
    "# 채팅 세션 시작\n",
    "chat_session = model.start_chat(\n",
    "  history=[]  # 대화 이력을 여기에 넣을 수 있습니다\n",
    ")\n",
    "\n",
    "# 메시지 전송 및 응답 받기\n",
    "response = chat_session.send_message(\"\")\n",
    "\n",
    "# 응답 출력\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i62wRU5g6i-l"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WAP4y9Um6jKV"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMvWu9yr7vy75c5Ya/YRMq9",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
